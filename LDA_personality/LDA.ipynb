{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading musings...\n",
      "Making bigrams...\n",
      "Making trigrams...\n",
      "Modding n-grams...\n",
      "Destroying stop words...\n",
      "Forming bigrams...\n",
      "Building spacy NLP parser...\n",
      "Lemmatizing bigrams...\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Null bigram found\n",
      "Building dictionary...\n",
      "Making corpus...\n",
      "Generating LDA Model...\n",
      "('\\nPerplexity: ', -153.55239074159076)\n",
      "('\\nCoherence Score: ', 0.4758500805026282)\n",
      "[(88,\n",
      "  u'0.422*\"part\" + 0.098*\"note\" + 0.079*\"series\" + 0.066*\"unix\" + 0.042*\"embarrass\" + 0.030*\"entitle\" + 0.028*\"ongoing_serie\" + 0.015*\"strategy\" + 0.013*\"essays_tentatively\" + 0.013*\"inhabit\"'),\n",
      " (16,\n",
      "  u'0.372*\"grinnell\" + 0.042*\"doug\" + 0.033*\"https_www\" + 0.031*\"curmudgeon\" + 0.029*\"recently\" + 0.023*\"admission\" + 0.021*\"org\" + 0.020*\"compare\" + 0.019*\"https\" + 0.017*\"digital\"'),\n",
      " (113,\n",
      "  u'0.171*\"understand\" + 0.120*\"professional\" + 0.057*\"increase\" + 0.039*\"lead\" + 0.038*\"personal\" + 0.037*\"compute\" + 0.032*\"human\" + 0.029*\"society\" + 0.026*\"agree\" + 0.026*\"service\"'),\n",
      " (98,\n",
      "  u'0.403*\"student\" + 0.055*\"prospective\" + 0.054*\"current\" + 0.047*\"parent\" + 0.044*\"audience\" + 0.028*\"concept\" + 0.022*\"scholar\" + 0.017*\"pay_attention\" + 0.016*\"version_releas\" + 0.016*\"generation\"'),\n",
      " (48,\n",
      "  u'0.166*\"grade\" + 0.118*\"exam\" + 0.073*\"lab\" + 0.063*\"assignment\" + 0.047*\"average\" + 0.045*\"final\" + 0.043*\"quiz\" + 0.023*\"homework\" + 0.021*\"scale\" + 0.020*\"rebelsky\"'),\n",
      " (117,\n",
      "  u'0.181*\"grinnell\" + 0.143*\"curriculum\" + 0.133*\"student\" + 0.095*\"general\" + 0.052*\"detail\" + 0.036*\"future\" + 0.036*\"cover\" + 0.032*\"requirement\" + 0.023*\"expense\" + 0.016*\"successful\"'),\n",
      " (58,\n",
      "  u'0.222*\"send\" + 0.216*\"email\" + 0.126*\"message\" + 0.039*\"reply\" + 0.036*\"receive\" + 0.034*\"respond\" + 0.033*\"followup\" + 0.027*\"request\" + 0.021*\"asa\" + 0.018*\"recipient\"'),\n",
      " (89,\n",
      "  u'0.106*\"document\" + 0.054*\"analysis\" + 0.048*\"date\" + 0.045*\"creative\" + 0.040*\"request\" + 0.026*\"guide\" + 0.026*\"context\" + 0.025*\"critical\" + 0.023*\"grinco\" + 0.022*\"wall\"'),\n",
      " (31,\n",
      "  u'0.435*\"make\" + 0.110*\"happy\" + 0.072*\"difficult\" + 0.031*\"sad\" + 0.022*\"writer\" + 0.021*\"hold\" + 0.018*\"competent\" + 0.016*\"angry\" + 0.015*\"force\" + 0.014*\"mistake\"'),\n",
      " (102,\n",
      "  u'0.130*\"level\" + 0.067*\"close\" + 0.061*\"benefit\" + 0.058*\"generally\" + 0.039*\"even_though\" + 0.038*\"risk\" + 0.022*\"importantly\" + 0.021*\"top\" + 0.018*\"banana\" + 0.017*\"contrast\"'),\n",
      " (49,\n",
      "  u'0.599*\"work\" + 0.236*\"hard\" + 0.007*\"delegate\" + 0.004*\"senior\" + 0.004*\"adjust\" + 0.004*\"note\" + 0.003*\"bear\" + 0.003*\"critique\" + 0.003*\"taste\" + 0.003*\"feedback\"'),\n",
      " (73,\n",
      "  u'0.162*\"bad\" + 0.141*\"open\" + 0.103*\"idea\" + 0.069*\"sound\" + 0.063*\"tag\" + 0.041*\"select\" + 0.039*\"theme\" + 0.035*\"choose\" + 0.022*\"equally\" + 0.018*\"textbook\"'),\n",
      " (61,\n",
      "  u'0.165*\"happen\" + 0.165*\"reason\" + 0.107*\"assume\" + 0.069*\"whatev\" + 0.069*\"thing\" + 0.061*\"miss\" + 0.060*\"good\" + 0.048*\"trust\" + 0.045*\"switch\" + 0.011*\"slow\"'),\n",
      " (136,\n",
      "  u'0.603*\"class\" + 0.040*\"session\" + 0.033*\"prepare\" + 0.018*\"news\" + 0.018*\"swear\" + 0.016*\"upper_level\" + 0.014*\"create\" + 0.013*\"collaboratively\" + 0.012*\"advance\" + 0.012*\"field\"'),\n",
      " (111,\n",
      "  u'0.158*\"position\" + 0.121*\"role\" + 0.103*\"play\" + 0.073*\"lose\" + 0.064*\"important\" + 0.051*\"proposal\" + 0.045*\"serve\" + 0.037*\"game\" + 0.030*\"leadership\" + 0.020*\"innovation\"'),\n",
      " (66,\n",
      "  u'0.227*\"software\" + 0.200*\"design\" + 0.064*\"development\" + 0.051*\"base\" + 0.048*\"tool\" + 0.035*\"rely\" + 0.022*\"environment\" + 0.016*\"dar\" + 0.014*\"aspect\" + 0.012*\"alumni_relation\"'),\n",
      " (51,\n",
      "  u'0.206*\"meeting\" + 0.102*\"department\" + 0.067*\"discuss\" + 0.064*\"minute\" + 0.054*\"key\" + 0.053*\"faculty\" + 0.044*\"division\" + 0.034*\"dean\" + 0.034*\"classroom\" + 0.027*\"weekly\"'),\n",
      " (42,\n",
      "  u'0.235*\"file\" + 0.057*\"command\" + 0.053*\"create\" + 0.049*\"directory\" + 0.034*\"type\" + 0.034*\"permission\" + 0.033*\"strange\" + 0.027*\"text\" + 0.018*\"user\" + 0.017*\"content\"'),\n",
      " (143,\n",
      "  u'0.303*\"find\" + 0.247*\"hope\" + 0.089*\"easy\" + 0.038*\"tomorrow\" + 0.031*\"trouble\" + 0.031*\"continue\" + 0.017*\"helpful\" + 0.016*\"year\" + 0.009*\"factor\" + 0.008*\"stay_tun\"'),\n",
      " (86,\n",
      "  u'0.114*\"team\" + 0.056*\"man\" + 0.042*\"woman\" + 0.038*\"schedule\" + 0.036*\"football\" + 0.036*\"grinnell\" + 0.034*\"play\" + 0.033*\"track\" + 0.030*\"field\" + 0.023*\"athletic\"')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0581</td>\n",
       "      <td>faculty, member, hire, senior, tenure, henry, ...</td>\n",
       "      <td>On Tuesday at lunch, I'm part of a small group...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>write, essay, short, sit, recommendation_lette...</td>\n",
       "      <td>I think this is one of those times that I need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>143</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>find, hope, easy, tomorrow, trouble, continue,...</td>\n",
       "      <td>I hope I don't get myself in too much trouble.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>college, institution, education, grinnell, lib...</td>\n",
       "      <td>Shared governance is a core aspect of higher e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0296</td>\n",
       "      <td>faculty, member, staff, salary, handbook, earl...</td>\n",
       "      <td>Its basic premise should be simple: The facult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>0.0404</td>\n",
       "      <td>make, difference, decision, choice, bad, huge,...</td>\n",
       "      <td>That is, they make decisions collaboratively w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0710</td>\n",
       "      <td>faculty, member, hire, senior, tenure, henry, ...</td>\n",
       "      <td>At times, collaboration may take the form on c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>138</td>\n",
       "      <td>0.0424</td>\n",
       "      <td>policy, grinnell, president, posse, relationsh...</td>\n",
       "      <td>Shared governance usually requires some form o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>85</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>committee, serve, sepc, purpose, stand, repres...</td>\n",
       "      <td>That is, because it is complicated to delibera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0444</td>\n",
       "      <td>group, share, include, associate, acm, ethic, ...</td>\n",
       "      <td>Those representative groups then have a respon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>138</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>policy, grinnell, president, posse, relationsh...</td>\n",
       "      <td>Successful shared governance is open governance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0358</td>\n",
       "      <td>hear, alum, talk, colleague, interested, perso...</td>\n",
       "      <td>Representative groups cannot communicate the w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>138</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>policy, grinnell, president, posse, relationsh...</td>\n",
       "      <td>Whenever possible (and it's not always possibl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>138</td>\n",
       "      <td>0.0821</td>\n",
       "      <td>policy, grinnell, president, posse, relationsh...</td>\n",
       "      <td>Admittedly, shared governance includes some ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>case, refer, start, typically, step, compiler,...</td>\n",
       "      <td>Ideally, in both case that \"ownership\" is stil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>grinnell, doug, https_www, curmudgeon, recentl...</td>\n",
       "      <td>I am concerned about the current status of sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>121</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>make, difference, decision, choice, bad, huge,...</td>\n",
       "      <td>Why?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>good, practice, generally, pretty, evidence, f...</td>\n",
       "      <td>First, we have had difficulties with transpare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>121</td>\n",
       "      <td>0.0331</td>\n",
       "      <td>make, difference, decision, choice, bad, huge,...</td>\n",
       "      <td>We've had that problem for some time, and it d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>number, large, small, amount, reasonable, roun...</td>\n",
       "      <td>Second, and perhaps more importantly, decision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>121</td>\n",
       "      <td>0.0424</td>\n",
       "      <td>make, difference, decision, choice, bad, huge,...</td>\n",
       "      <td>Why do I think that?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>number, large, small, amount, reasonable, roun...</td>\n",
       "      <td>It's not because of the Posse decision [1], al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>number, large, small, amount, reasonable, roun...</td>\n",
       "      <td>It's because of the slew of small and not so s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>85</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>committee, serve, sepc, purpose, stand, repres...</td>\n",
       "      <td>Let's consider some examples.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>85</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>committee, serve, sepc, purpose, stand, repres...</td>\n",
       "      <td>The Faculty Organization Committee (FOC) [2] i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>85</td>\n",
       "      <td>0.0547</td>\n",
       "      <td>committee, serve, sepc, purpose, stand, repres...</td>\n",
       "      <td>In the past few years, I've seen the administr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0758</td>\n",
       "      <td>change, make, significant, significantly, atte...</td>\n",
       "      <td>The Instructional Support Committee (ISC) is r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>82</td>\n",
       "      <td>0.0390</td>\n",
       "      <td>discussion, reading, bookstore, advisee, recen...</td>\n",
       "      <td>But changes to the Web site were made without ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>54</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>science, art, focus, social, complex, center, ...</td>\n",
       "      <td>ISC's responsibilities are also supposed to in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>82</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>discussion, reading, bookstore, advisee, recen...</td>\n",
       "      <td>In particular, ISC is supposed to \"raise issue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44552</th>\n",
       "      <td>44552</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3] And it often does.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44553</th>\n",
       "      <td>44553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[4] You may be surprised to hear that if a stu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44554</th>\n",
       "      <td>44554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[5] Grinnell College (2017).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44555</th>\n",
       "      <td>44555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Grinnell College 2017-2018 Student Handbook, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44556</th>\n",
       "      <td>44556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[6] I found this in a section of the page that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44557</th>\n",
       "      <td>44557</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>But the handbook and catalog links do not seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44558</th>\n",
       "      <td>44558</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7] Grinnell College Academic Advising (2018).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44559</th>\n",
       "      <td>44559</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adviser's Handbook.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44560</th>\n",
       "      <td>44560</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Section 1: Nuts and Bolts: Graduating on Time:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44561</th>\n",
       "      <td>44561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Found on the Web at http://www.grinnell.edu/ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44562</th>\n",
       "      <td>44562</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[8] Email from Dean's office, 4 April 2018.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44563</th>\n",
       "      <td>44563</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[9] Mathematics and Statistics share a departm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44564</th>\n",
       "      <td>44564</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Will we see a similar split for those topics?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44565</th>\n",
       "      <td>44565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Given my experience with the faculty's respons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44566</th>\n",
       "      <td>44566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>French and Arabic also share a department.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44567</th>\n",
       "      <td>44567</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I wonder whether Arabic courses count toward t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44568</th>\n",
       "      <td>44568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[10] Samuel A. Rebelsky.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44569</th>\n",
       "      <td>44569</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Getting requirements wrong\".</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44570</th>\n",
       "      <td>44570</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Online document at https://www.cs.grinnell.edu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44571</th>\n",
       "      <td>44571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[11] It may be that Grinnell does not apply th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44572</th>\n",
       "      <td>44572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>But it's a joint department of French and Arabic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44573</th>\n",
       "      <td>44573</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And I don't see an explicit exemption for them...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44574</th>\n",
       "      <td>44574</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[12] Of course, many majors have hidden requir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44575</th>\n",
       "      <td>44575</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I keep finding new ones each time I look.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44576</th>\n",
       "      <td>44576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>For example, in writing this musing, I learned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44577</th>\n",
       "      <td>44577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Studio Art doesn't count it either, but does r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44578</th>\n",
       "      <td>44578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[14] More about that issue in another musing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44579</th>\n",
       "      <td>44579</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[15] The answer from one of my readers is \"Yes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44580</th>\n",
       "      <td>44580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[16] Well, there's Theatre and Dance, but they...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44581</th>\n",
       "      <td>44581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Version 1.0 releaed 2018-03-07.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44582 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0                0              19              0.0581   \n",
       "1                1              90              0.0238   \n",
       "2                2             143              0.0385   \n",
       "3                3               4              0.0432   \n",
       "4                4              75              0.0296   \n",
       "5                5             121              0.0404   \n",
       "6                6              19              0.0710   \n",
       "7                7             138              0.0424   \n",
       "8                8              85              0.0394   \n",
       "9                9              40              0.0444   \n",
       "10              10             138              0.0432   \n",
       "11              11              60              0.0358   \n",
       "12              12             138              0.0417   \n",
       "13              13             138              0.0821   \n",
       "14              14              25              0.0370   \n",
       "15              15              16              0.0283   \n",
       "16              16             121              0.0613   \n",
       "17              17              17              0.0252   \n",
       "18              18             121              0.0331   \n",
       "19              19               0              0.0067   \n",
       "20              20             121              0.0424   \n",
       "21              21               0              0.0273   \n",
       "22              22               0              0.0067   \n",
       "23              23              85              0.0881   \n",
       "24              24              85              0.0481   \n",
       "25              25              85              0.0547   \n",
       "26              26               7              0.0758   \n",
       "27              27              82              0.0390   \n",
       "28              28              54              0.0489   \n",
       "29              29              82              0.0772   \n",
       "...            ...             ...                 ...   \n",
       "44552        44552             NaN                 NaN   \n",
       "44553        44553             NaN                 NaN   \n",
       "44554        44554             NaN                 NaN   \n",
       "44555        44555             NaN                 NaN   \n",
       "44556        44556             NaN                 NaN   \n",
       "44557        44557             NaN                 NaN   \n",
       "44558        44558             NaN                 NaN   \n",
       "44559        44559             NaN                 NaN   \n",
       "44560        44560             NaN                 NaN   \n",
       "44561        44561             NaN                 NaN   \n",
       "44562        44562             NaN                 NaN   \n",
       "44563        44563             NaN                 NaN   \n",
       "44564        44564             NaN                 NaN   \n",
       "44565        44565             NaN                 NaN   \n",
       "44566        44566             NaN                 NaN   \n",
       "44567        44567             NaN                 NaN   \n",
       "44568        44568             NaN                 NaN   \n",
       "44569        44569             NaN                 NaN   \n",
       "44570        44570             NaN                 NaN   \n",
       "44571        44571             NaN                 NaN   \n",
       "44572        44572             NaN                 NaN   \n",
       "44573        44573             NaN                 NaN   \n",
       "44574        44574             NaN                 NaN   \n",
       "44575        44575             NaN                 NaN   \n",
       "44576        44576             NaN                 NaN   \n",
       "44577        44577             NaN                 NaN   \n",
       "44578        44578             NaN                 NaN   \n",
       "44579        44579             NaN                 NaN   \n",
       "44580        44580             NaN                 NaN   \n",
       "44581        44581             NaN                 NaN   \n",
       "\n",
       "                                                Keywords  \\\n",
       "0      faculty, member, hire, senior, tenure, henry, ...   \n",
       "1      write, essay, short, sit, recommendation_lette...   \n",
       "2      find, hope, easy, tomorrow, trouble, continue,...   \n",
       "3      college, institution, education, grinnell, lib...   \n",
       "4      faculty, member, staff, salary, handbook, earl...   \n",
       "5      make, difference, decision, choice, bad, huge,...   \n",
       "6      faculty, member, hire, senior, tenure, henry, ...   \n",
       "7      policy, grinnell, president, posse, relationsh...   \n",
       "8      committee, serve, sepc, purpose, stand, repres...   \n",
       "9      group, share, include, associate, acm, ethic, ...   \n",
       "10     policy, grinnell, president, posse, relationsh...   \n",
       "11     hear, alum, talk, colleague, interested, perso...   \n",
       "12     policy, grinnell, president, posse, relationsh...   \n",
       "13     policy, grinnell, president, posse, relationsh...   \n",
       "14     case, refer, start, typically, step, compiler,...   \n",
       "15     grinnell, doug, https_www, curmudgeon, recentl...   \n",
       "16     make, difference, decision, choice, bad, huge,...   \n",
       "17     good, practice, generally, pretty, evidence, f...   \n",
       "18     make, difference, decision, choice, bad, huge,...   \n",
       "19     number, large, small, amount, reasonable, roun...   \n",
       "20     make, difference, decision, choice, bad, huge,...   \n",
       "21     number, large, small, amount, reasonable, roun...   \n",
       "22     number, large, small, amount, reasonable, roun...   \n",
       "23     committee, serve, sepc, purpose, stand, repres...   \n",
       "24     committee, serve, sepc, purpose, stand, repres...   \n",
       "25     committee, serve, sepc, purpose, stand, repres...   \n",
       "26     change, make, significant, significantly, atte...   \n",
       "27     discussion, reading, bookstore, advisee, recen...   \n",
       "28     science, art, focus, social, complex, center, ...   \n",
       "29     discussion, reading, bookstore, advisee, recen...   \n",
       "...                                                  ...   \n",
       "44552                                                NaN   \n",
       "44553                                                NaN   \n",
       "44554                                                NaN   \n",
       "44555                                                NaN   \n",
       "44556                                                NaN   \n",
       "44557                                                NaN   \n",
       "44558                                                NaN   \n",
       "44559                                                NaN   \n",
       "44560                                                NaN   \n",
       "44561                                                NaN   \n",
       "44562                                                NaN   \n",
       "44563                                                NaN   \n",
       "44564                                                NaN   \n",
       "44565                                                NaN   \n",
       "44566                                                NaN   \n",
       "44567                                                NaN   \n",
       "44568                                                NaN   \n",
       "44569                                                NaN   \n",
       "44570                                                NaN   \n",
       "44571                                                NaN   \n",
       "44572                                                NaN   \n",
       "44573                                                NaN   \n",
       "44574                                                NaN   \n",
       "44575                                                NaN   \n",
       "44576                                                NaN   \n",
       "44577                                                NaN   \n",
       "44578                                                NaN   \n",
       "44579                                                NaN   \n",
       "44580                                                NaN   \n",
       "44581                                                NaN   \n",
       "\n",
       "                                                    Text  \n",
       "0      On Tuesday at lunch, I'm part of a small group...  \n",
       "1      I think this is one of those times that I need...  \n",
       "2         I hope I don't get myself in too much trouble.  \n",
       "3      Shared governance is a core aspect of higher e...  \n",
       "4      Its basic premise should be simple: The facult...  \n",
       "5      That is, they make decisions collaboratively w...  \n",
       "6      At times, collaboration may take the form on c...  \n",
       "7      Shared governance usually requires some form o...  \n",
       "8      That is, because it is complicated to delibera...  \n",
       "9      Those representative groups then have a respon...  \n",
       "10      Successful shared governance is open governance.  \n",
       "11     Representative groups cannot communicate the w...  \n",
       "12     Whenever possible (and it's not always possibl...  \n",
       "13     Admittedly, shared governance includes some ac...  \n",
       "14     Ideally, in both case that \"ownership\" is stil...  \n",
       "15     I am concerned about the current status of sha...  \n",
       "16                                                  Why?  \n",
       "17     First, we have had difficulties with transpare...  \n",
       "18     We've had that problem for some time, and it d...  \n",
       "19     Second, and perhaps more importantly, decision...  \n",
       "20                                  Why do I think that?  \n",
       "21     It's not because of the Posse decision [1], al...  \n",
       "22     It's because of the slew of small and not so s...  \n",
       "23                         Let's consider some examples.  \n",
       "24     The Faculty Organization Committee (FOC) [2] i...  \n",
       "25     In the past few years, I've seen the administr...  \n",
       "26     The Instructional Support Committee (ISC) is r...  \n",
       "27     But changes to the Web site were made without ...  \n",
       "28     ISC's responsibilities are also supposed to in...  \n",
       "29     In particular, ISC is supposed to \"raise issue...  \n",
       "...                                                  ...  \n",
       "44552                             [3] And it often does.  \n",
       "44553  [4] You may be surprised to hear that if a stu...  \n",
       "44554                       [5] Grinnell College (2017).  \n",
       "44555  Grinnell College 2017-2018 Student Handbook, A...  \n",
       "44556  [6] I found this in a section of the page that...  \n",
       "44557  But the handbook and catalog links do not seem...  \n",
       "44558     [7] Grinnell College Academic Advising (2018).  \n",
       "44559                                Adviser's Handbook.  \n",
       "44560  Section 1: Nuts and Bolts: Graduating on Time:...  \n",
       "44561  Found on the Web at http://www.grinnell.edu/ab...  \n",
       "44562        [8] Email from Dean's office, 4 April 2018.  \n",
       "44563  [9] Mathematics and Statistics share a departm...  \n",
       "44564      Will we see a similar split for those topics?  \n",
       "44565  Given my experience with the faculty's respons...  \n",
       "44566         French and Arabic also share a department.  \n",
       "44567  I wonder whether Arabic courses count toward t...  \n",
       "44568                           [10] Samuel A. Rebelsky.  \n",
       "44569                      \"Getting requirements wrong\".  \n",
       "44570  Online document at https://www.cs.grinnell.edu...  \n",
       "44571  [11] It may be that Grinnell does not apply th...  \n",
       "44572  But it's a joint department of French and Arabic.  \n",
       "44573  And I don't see an explicit exemption for them...  \n",
       "44574  [12] Of course, many majors have hidden requir...  \n",
       "44575          I keep finding new ones each time I look.  \n",
       "44576  For example, in writing this musing, I learned...  \n",
       "44577  Studio Art doesn't count it either, but does r...  \n",
       "44578      [14] More about that issue in another musing.  \n",
       "44579  [15] The answer from one of my readers is \"Yes...  \n",
       "44580  [16] Well, there's Theatre and Dance, but they...  \n",
       "44581                    Version 1.0 releaed 2018-03-07.  \n",
       "\n",
       "[44582 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "stop_words = stopwords.words('english')\n",
    "# stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "musings_path = 'sam_musings/'\n",
    "musings = os.listdir(musings_path)\n",
    "\n",
    "# Import Dataset\n",
    "print('Loading musings...')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "musings = '\\n'.join([open(musings_path + musing).read() for musing in musings])\n",
    "musing_sentences = tokenizer.tokenize(musings)\n",
    "df = pd.DataFrame([{'content': sentence} for sentence in musing_sentences])\n",
    "\n",
    "# Convert to list\n",
    "data = df.content.values.tolist()\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "print('Making bigrams...')\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "\n",
    "print('Making trigrams...')\n",
    "#trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "print('Modding n-grams...')\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "#trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        try:\n",
    "            doc = nlp(\" \".join(sent)) \n",
    "            texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        except TypeError:\n",
    "            if len(sent) == 0:\n",
    "                print 'Null bigram found'\n",
    "            else:\n",
    "                print 'something is wrong with bigram: {}'.format(sent)\n",
    "    return texts_out\n",
    "\n",
    "# Remove Stop Words\n",
    "print('Destroying stop words...')\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "print('Forming bigrams...')\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "print('Building spacy NLP parser...')\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "print('Lemmatizing bigrams...')\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# Create Dictionary\n",
    "print('Building dictionary...')\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "print('Making corpus...')\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "topic_sizes = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100, 200]\n",
    "lda_models = [] * len(topic_sizes)\n",
    "coherence_model_ldas = [] * len(topic_sizes)\n",
    "coherence_lda = np.zeros(len(topic_sizes))\n",
    "\n",
    "# Build LDA model\n",
    "print('Generating LDA Model...')\n",
    "#for index, topics in enumerate(topic_sizes):\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                   id2word=id2word,\n",
    "                                                   num_topics=250,\n",
    "                                                   random_state=100,\n",
    "                                                   update_every=1,\n",
    "                                                   chunksize=100,\n",
    "                                                   passes=10,\n",
    "                                                   alpha='auto',\n",
    "                                                   per_word_topics=True)\n",
    "    \n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "doc_lda = lda_model[corpus]\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "# Generate mallet topic\n",
    "mallet_path = '../mallet-2.0.8/bin/mallet' # update this path\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=150, id2word=id2word)\n",
    "\n",
    "\"\"\"\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "\n",
    "limit=150; start=50; step=10;\n",
    "\n",
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=start, limit=limit, step=step)\n",
    "\n",
    "# Show graph\n",
    "x = range(start, limit, step)\n",
    "\n",
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n",
    "\"\"\"\n",
    "    \n",
    "# Select the model and print the topics\n",
    "#optimal_model = model_list[np.argmax(coherence_values)]\n",
    "optimal_model = ldamallet\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))\n",
    "\n",
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_sorteddf_mallet['Text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopicForQuery(question, model=lda_model):\n",
    "    temp = question.lower()\n",
    "\n",
    "    words = re.findall(r'\\w+', temp, flags = re.UNICODE | re.LOCALE)\n",
    "    \n",
    "    important_words = [word for word in simple_preprocess(str(words)) if word not in stop_words]\n",
    "    \n",
    "    dictionary = corpora.Dictionary.load('sam_dict.dict')\n",
    "\n",
    "    ques_vec = dictionary.doc2bow(important_words)\n",
    "\n",
    "    topic_vec = model[[ques_vec]]\n",
    "\n",
    "    likely_topic = max(topic_vec[0], key=itemgetter(1))[0]\n",
    "    \n",
    "    return likely_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dominant_topic = getTopicForQuery(\"Good morning Sam Bot.\", model=optimal_model)\n",
    "related_sentences = df_dominant_topic[df_dominant_topic['Dominant_Topic'] == query_dominant_topic].reset_index(drop=True)\n",
    "related_sentences.loc[random.randint(0,len(related_sentences))]['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word.save('sam_dict.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.get_topics().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[4].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[score for topic_id, score in topic] for topic in [doc for doc in lda_corpus]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.print_topic(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.to_csv('sam_dominant_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model.save('sam_lda_model.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word.save('sam_dict.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-204e5af1e7ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'models'"
     ]
    }
   ],
   "source": [
    "gensim.models.wrappers.LdaMallet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LdaMallet in module gensim.models.wrappers.ldamallet:\n",
      "\n",
      "class LdaMallet(gensim.utils.SaveLoad, gensim.models.basemodel.BaseTopicModel)\n",
      " |  Python wrapper for LDA using `MALLET <http://mallet.cs.umass.edu/>`_.\n",
      " |  \n",
      " |  Communication between MALLET and Python takes place by passing around data files on disk\n",
      " |  and calling Java with subprocess.call().\n",
      " |  \n",
      " |  Warnings\n",
      " |  --------\n",
      " |  This is **only** python wrapper for `MALLET LDA <http://mallet.cs.umass.edu/>`_,\n",
      " |  you need to install original implementation first and pass the path to binary to ``mallet_path``.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LdaMallet\n",
      " |      gensim.utils.SaveLoad\n",
      " |      gensim.models.basemodel.BaseTopicModel\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, bow, iterations=100)\n",
      " |      Get vector for document(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      bow : {list of (int, int), iterable of list of (int, int)}\n",
      " |          Document (or corpus) in BoW format.\n",
      " |      iterations : int, optional\n",
      " |          Number of iterations that will be used for inferring.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, float)\n",
      " |          LDA vector for document as sequence of (topic_id, topic_probability) **OR**\n",
      " |      list of list of (int, float)\n",
      " |          LDA vectors for corpus in same format.\n",
      " |  \n",
      " |  __init__(self, mallet_path, corpus=None, num_topics=100, alpha=50, id2word=None, workers=4, prefix=None, optimize_interval=0, iterations=1000, topic_threshold=0.0)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      mallet_path : str\n",
      " |          Path to the mallet binary, e.g. `/home/username/mallet-2.0.7/bin/mallet`.\n",
      " |      corpus : iterable of iterable of (int, int), optional\n",
      " |          Collection of texts in BoW format.\n",
      " |      num_topics : int, optional\n",
      " |          Number of topics.\n",
      " |      alpha : int, optional\n",
      " |          Alpha parameter of LDA.\n",
      " |      id2word : :class:`~gensim.corpora.dictionary.Dictionary`, optional\n",
      " |          Mapping between tokens ids and words from corpus, if not specified - will be inferred from `corpus`.\n",
      " |      workers : int, optional\n",
      " |          Number of threads that will be used for training.\n",
      " |      prefix : str, optional\n",
      " |          Prefix for produced temporary files.\n",
      " |      optimize_interval : int, optional\n",
      " |          Optimize hyperparameters every `optimize_interval` iterations\n",
      " |          (sometimes leads to Java exception 0 to switch off hyperparameter optimization).\n",
      " |      iterations : int, optional\n",
      " |          Number of training iterations.\n",
      " |      topic_threshold : float, optional\n",
      " |          Threshold of the probability above which we consider a topic.\n",
      " |  \n",
      " |  convert_input(self, corpus, infer=False, serialize_corpus=True)\n",
      " |      Convert corpus to Mallet format and save it to a temporary text file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : iterable of iterable of (int, int)\n",
      " |          Collection of texts in BoW format.\n",
      " |      infer : bool, optional\n",
      " |          ...\n",
      " |      serialize_corpus : bool, optional\n",
      " |          ...\n",
      " |  \n",
      " |  corpus2mallet(self, corpus, file_like)\n",
      " |      Convert `corpus` to Mallet format and write it to `file_like` descriptor.\n",
      " |      \n",
      " |      Format ::\n",
      " |      \n",
      " |          document id[SPACE]label (not used)[SPACE]whitespace delimited utf8-encoded tokens[NEWLINE]\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : iterable of iterable of (int, int)\n",
      " |          Collection of texts in BoW format.\n",
      " |      file_like : file-like object\n",
      " |          Opened file.\n",
      " |  \n",
      " |  fcorpusmallet(self)\n",
      " |      Get path to corpus.mallet file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Path to corpus.mallet file.\n",
      " |  \n",
      " |  fcorpustxt(self)\n",
      " |      Get path to corpus text file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Path to corpus text file.\n",
      " |  \n",
      " |  fdoctopics(self)\n",
      " |      Get path to document topic text file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Path to document topic text file.\n",
      " |  \n",
      " |  finferencer(self)\n",
      " |      Get path to inferencer.mallet file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Path to inferencer.mallet file.\n",
      " |  \n",
      " |  fstate(self)\n",
      " |      Get path to temporary file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Path to file.\n",
      " |  \n",
      " |  ftopickeys(self)\n",
      " |      Get path to topic keys text file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Path to topic keys text file.\n",
      " |  \n",
      " |  fwordweights(self)\n",
      " |      Get path to word weight file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Path to word weight file.\n",
      " |  \n",
      " |  get_topics(self)\n",
      " |      Get topics X words matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Topics X words matrix, shape `num_topics` x `vocabulary_size`.\n",
      " |  \n",
      " |  get_version(self, direc_path)\n",
      " |      \"Get the version of Mallet.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      direc_path : str\n",
      " |          Path to mallet archive.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Version of mallet.\n",
      " |  \n",
      " |  load_document_topics(self)\n",
      " |      Load document topics from :meth:`gensim.models.wrappers.ldamallet.LdaMallet.fdoctopics` file.\n",
      " |      Shortcut for :meth:`gensim.models.wrappers.ldamallet.LdaMallet.read_doctopics`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      iterator of list of (int, float)\n",
      " |          Sequence of LDA vectors for documents.\n",
      " |  \n",
      " |  load_word_topics(self)\n",
      " |      Load words X topics matrix from :meth:`gensim.models.wrappers.ldamallet.LdaMallet.fstate` file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Matrix words X topics.\n",
      " |  \n",
      " |  read_doctopics(self, fname, eps=1e-06, renorm=True)\n",
      " |      Get document topic vectors from MALLET's \"doc-topics\" format, as sparse gensim vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to input file with document topics.\n",
      " |      eps : float, optional\n",
      " |          Threshold for probabilities.\n",
      " |      renorm : bool, optional\n",
      " |          If True - explicitly re-normalize distribution.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      RuntimeError\n",
      " |          If any line in invalid format.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      list of (int, float)\n",
      " |          LDA vectors for document.\n",
      " |  \n",
      " |  show_topic(self, topicid, topn=10, num_words=None)\n",
      " |      Get `num_words` most probable words for the given `topicid`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topicid : int\n",
      " |          Id of topic.\n",
      " |      topn : int, optional\n",
      " |          Top number of topics that you'll receive.\n",
      " |      num_words : int, optional\n",
      " |          DEPRECATED PARAMETER, use `topn` instead.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float)\n",
      " |          Sequence of probable words, as a list of `(word, word_probability)` for `topicid` topic.\n",
      " |  \n",
      " |  show_topics(self, num_topics=10, num_words=10, log=False, formatted=True)\n",
      " |      Get the `num_words` most probable words for `num_topics` number of topics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num_topics : int, optional\n",
      " |          Number of topics to return, set `-1` to get all topics.\n",
      " |      num_words : int, optional\n",
      " |          Number of words.\n",
      " |      log : bool, optional\n",
      " |          If True - write topic with logging too, used for debug proposes.\n",
      " |      formatted : bool, optional\n",
      " |          If `True` - return the topics as a list of strings, otherwise as lists of (weight, word) pairs.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of str\n",
      " |          Topics as a list of strings (if formatted=True) **OR**\n",
      " |      list of (float, str)\n",
      " |          Topics as list of (weight, word) pairs (if formatted=False)\n",
      " |  \n",
      " |  train(self, corpus)\n",
      " |      Train Mallet LDA.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : iterable of iterable of (int, int)\n",
      " |          Corpus in BoW format\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __slotnames__ = []\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset([]), pickle_protocol=2)\n",
      " |      Save the object to file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname_or_handle : str or file-like\n",
      " |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      " |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      " |      separately : list of str or None, optional\n",
      " |          If None -  automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      " |          them into separate files. This avoids pickle memory errors and allows mmap'ing large arrays\n",
      " |          back on load efficiently.\n",
      " |          If list of str - this attributes will be stored in separate files, the automatic check\n",
      " |          is not performed in this case.\n",
      " |      sep_limit : int\n",
      " |          Limit for automatic separation.\n",
      " |      ignore : frozenset of str\n",
      " |          Attributes that shouldn't be serialize/store.\n",
      " |      pickle_protocol : int\n",
      " |          Protocol number for pickle.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.load`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  load(cls, fname, mmap=None) from __builtin__.type\n",
      " |      Load a previously saved object (using :meth:`~gensim.utils.SaveLoad.save`) from file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to file that contains needed object.\n",
      " |      mmap : str, optional\n",
      " |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      " |          via mmap (shared memory) using `mmap='r'.\n",
      " |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object\n",
      " |          Object loaded from `fname`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      IOError\n",
      " |          When methods are called on instance (should be called from class).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.basemodel.BaseTopicModel:\n",
      " |  \n",
      " |  print_topic(self, topicno, topn=10)\n",
      " |      Get a single topic as a formatted string.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topicno : int\n",
      " |          Topic id.\n",
      " |      topn : int\n",
      " |          Number of words from topic that will be used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          String representation of topic, like '-0.340 * \"category\" + 0.298 * \"$M$\" + 0.183 * \"algebra\" + ... '.\n",
      " |  \n",
      " |  print_topics(self, num_topics=20, num_words=10)\n",
      " |      Get the most significant topics (alias for `show_topics()` method).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num_topics : int, optional\n",
      " |          The number of topics to be selected, if -1 - all topics will be in result (ordered by significance).\n",
      " |      num_words : int, optional\n",
      " |          The number of words to be included per topics (ordered by significance).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, list of (str, float))\n",
      " |          Sequence with (topic_id, [(word, value), ... ]).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gensim.models.wrappers.LdaMallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
