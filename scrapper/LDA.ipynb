{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading musings...\n",
      "Making bigrams...\n",
      "Making trigrams...\n",
      "Modding n-grams...\n",
      "Destroying stop words...\n",
      "Forming bigrams...\n",
      "Building spacy NLP parser...\n",
      "Lemmatizing bigrams...\n",
      "Building dictionary...\n",
      "Making corpus...\n",
      "Generating LDA Model...\n",
      "('\\nPerplexity: ', -7.101449534735451)\n",
      "('\\nCoherence Score: ', 0.3545563928835792)\n",
      "('Num Topics =', 32, ' has Coherence Value of', 0.4853)\n",
      "('Num Topics =', 38, ' has Coherence Value of', 0.4753)\n",
      "[(5,\n",
      "  u'0.125*\"feed\" + 0.111*\"rss\" + 0.069*\"title\" + 0.056*\"http_www\" + 0.049*\"file\" + 0.035*\"gmt\" + 0.035*\"content\" + 0.035*\"reader\" + 0.028*\"cgi\" + 0.028*\"cs\"'),\n",
      " (7,\n",
      "  u'0.207*\"grinnell\" + 0.089*\"variety\" + 0.074*\"cs\" + 0.074*\"problem\" + 0.052*\"technology\" + 0.044*\"plan\" + 0.030*\"break\" + 0.030*\"tool\" + 0.022*\"encourage\" + 0.022*\"build\"'),\n",
      " (11,\n",
      "  u'0.197*\"major\" + 0.091*\"thing\" + 0.061*\"add\" + 0.053*\"curriculum\" + 0.053*\"question\" + 0.045*\"feel\" + 0.045*\"topic\" + 0.038*\"graduate\" + 0.030*\"issue\" + 0.023*\"low\"'),\n",
      " (18,\n",
      "  u'0.188*\"teach\" + 0.076*\"design\" + 0.041*\"school\" + 0.035*\"part\" + 0.035*\"provide\" + 0.029*\"partnership\" + 0.029*\"project\" + 0.029*\"continue\" + 0.029*\"discussion\" + 0.024*\"teacher\"'),\n",
      " (10,\n",
      "  u'0.135*\"lot\" + 0.095*\"faculty\" + 0.095*\"give\" + 0.079*\"note\" + 0.056*\"exit_interview\" + 0.040*\"question\" + 0.040*\"senior\" + 0.040*\"relate\" + 0.032*\"grinnell\" + 0.024*\"find\"'),\n",
      " (17,\n",
      "  u'0.103*\"department\" + 0.097*\"time\" + 0.090*\"part\" + 0.071*\"end\" + 0.058*\"change\" + 0.058*\"year\" + 0.052*\"hear\" + 0.045*\"expect\" + 0.045*\"reason\" + 0.026*\"hope\"'),\n",
      " (19,\n",
      "  u'0.125*\"people\" + 0.073*\"professional\" + 0.062*\"fortunate\" + 0.062*\"discipline\" + 0.052*\"sigcse\" + 0.042*\"research\" + 0.042*\"conference\" + 0.042*\"reflect\" + 0.042*\"attend\" + 0.031*\"life\"'),\n",
      " (26,\n",
      "  u'0.151*\"good\" + 0.089*\"community\" + 0.068*\"essay\" + 0.055*\"comment\" + 0.048*\"education\" + 0.041*\"care\" + 0.034*\"move\" + 0.027*\"aspect\" + 0.027*\"pita\" + 0.027*\"time\"'),\n",
      " (8,\n",
      "  u'0.143*\"classic\" + 0.061*\"artist\" + 0.061*\"response\" + 0.041*\"late\" + 0.041*\"refer\" + 0.041*\"act\" + 0.041*\"newspaper\" + 0.041*\"proof\" + 0.020*\"dylan\" + 0.020*\"hana\"'),\n",
      " (6,\n",
      "  u'0.375*\"student\" + 0.129*\"work\" + 0.075*\"learn\" + 0.058*\"experience\" + 0.058*\"class\" + 0.021*\"interesting\" + 0.017*\"start\" + 0.017*\"benefit\" + 0.013*\"present\" + 0.013*\"call\"'),\n",
      " (3,\n",
      "  u'0.115*\"group\" + 0.067*\"isc\" + 0.058*\"include\" + 0.058*\"change\" + 0.058*\"perspective\" + 0.058*\"common\" + 0.038*\"represent\" + 0.038*\"representative\" + 0.038*\"planning\" + 0.029*\"consult\"'),\n",
      " (12,\n",
      "  u'0.096*\"structure\" + 0.067*\"serve\" + 0.058*\"critique\" + 0.058*\"system\" + 0.058*\"large\" + 0.048*\"admit\" + 0.038*\"background\" + 0.038*\"institution\" + 0.038*\"opportunity\" + 0.029*\"hour\"'),\n",
      " (24,\n",
      "  u'0.196*\"faculty\" + 0.070*\"decision\" + 0.056*\"committee\" + 0.056*\"governance\" + 0.049*\"administration\" + 0.049*\"dean\" + 0.042*\"discuss\" + 0.035*\"bookstore\" + 0.028*\"president\" + 0.028*\"campus\"'),\n",
      " (25,\n",
      "  u'0.085*\"interface\" + 0.085*\"text\" + 0.070*\"material\" + 0.056*\"thought\" + 0.056*\"script\" + 0.042*\"person\" + 0.042*\"strength\" + 0.042*\"slack\" + 0.042*\"type\" + 0.028*\"anxiety\"'),\n",
      " (0,\n",
      "  u'0.087*\"programming\" + 0.087*\"report\" + 0.058*\"hard\" + 0.058*\"significant\" + 0.058*\"valuable\" + 0.043*\"offer\" + 0.029*\"page\" + 0.029*\"show\" + 0.029*\"reflection\" + 0.029*\"aspect\"'),\n",
      " (29,\n",
      "  u'0.129*\"online\" + 0.124*\"goa\" + 0.053*\"week\" + 0.053*\"approach\" + 0.048*\"canvas\" + 0.038*\"face\" + 0.033*\"regularly\" + 0.029*\"video\" + 0.029*\"challenge\" + 0.024*\"lucas\"'),\n",
      " (2,\n",
      "  u'0.158*\"write\" + 0.079*\"question\" + 0.053*\"reading\" + 0.053*\"student\" + 0.039*\"enjoy\" + 0.026*\"education\" + 0.026*\"accessible\" + 0.026*\"friday\" + 0.026*\"advantage\" + 0.026*\"active\"'),\n",
      " (1,\n",
      "  u'0.102*\"musing\" + 0.080*\"link\" + 0.066*\"description\" + 0.058*\"edu_rebelsky\" + 0.058*\"xml\" + 0.058*\"guid\" + 0.051*\"pubdate\" + 0.051*\"item\" + 0.044*\"muse\" + 0.044*\"generate\"'),\n",
      " (23,\n",
      "  u'0.113*\"make\" + 0.083*\"issue\" + 0.077*\"support\" + 0.065*\"teaching\" + 0.048*\"high\" + 0.036*\"instructional\" + 0.030*\"involve\" + 0.024*\"staff\" + 0.024*\"put\" + 0.024*\"significantly\"'),\n",
      " (14,\n",
      "  u'0.185*\"find\" + 0.037*\"college\" + 0.037*\"longer\" + 0.037*\"overview\" + 0.037*\"application\" + 0.025*\"api\" + 0.025*\"hand\" + 0.025*\"broad\" + 0.025*\"format\" + 0.025*\"put\"')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0.2415</td>\n",
       "      <td>faculty, decision, committee, governance, admi...</td>\n",
       "      <td>On Tuesday at lunch, I'm part of a small group...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2927</td>\n",
       "      <td>feed, rss, title, http_www, file, gmt, content...</td>\n",
       "      <td>A variety of people have asked that I set up a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>0.5365</td>\n",
       "      <td>post, message, reply, original, list, quote, m...</td>\n",
       "      <td>In my copious spare time, I serve as one of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>major, thing, add, curriculum, question, feel,...</td>\n",
       "      <td>For as long as I've been a faculty member at G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.2474</td>\n",
       "      <td>classic, artist, response, late, refer, act, n...</td>\n",
       "      <td>On Friday, I was reading the Scarlet and Black...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>0.1845</td>\n",
       "      <td>online, goa, week, approach, canvas, face, reg...</td>\n",
       "      <td>This past semester, I've had my first experien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>0.1468</td>\n",
       "      <td>computer, session, program, talk, computer_sci...</td>\n",
       "      <td>Research evidence suggests that Grinnell secon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>0.1643</td>\n",
       "      <td>bad, academic, explore, fun, nighter, reality,...</td>\n",
       "      <td>I've been making some bad choices recently. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>0.2656</td>\n",
       "      <td>term, line, goal, kind, happen, request, bit, ...</td>\n",
       "      <td>I rant a lot about the administration. I there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>0.2986</td>\n",
       "      <td>people, professional, fortunate, discipline, s...</td>\n",
       "      <td>I just returned from the 2018 SIGCSE Technical...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0              24              0.2415   \n",
       "1            1               5              0.2927   \n",
       "2            2              30              0.5365   \n",
       "3            3              11              0.1800   \n",
       "4            4               8              0.2474   \n",
       "5            5              29              0.1845   \n",
       "6            6              13              0.1468   \n",
       "7            7              22              0.1643   \n",
       "8            8              15              0.2656   \n",
       "9            9              19              0.2986   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  faculty, decision, committee, governance, admi...   \n",
       "1  feed, rss, title, http_www, file, gmt, content...   \n",
       "2  post, message, reply, original, list, quote, m...   \n",
       "3  major, thing, add, curriculum, question, feel,...   \n",
       "4  classic, artist, response, late, refer, act, n...   \n",
       "5  online, goa, week, approach, canvas, face, reg...   \n",
       "6  computer, session, program, talk, computer_sci...   \n",
       "7  bad, academic, explore, fun, nighter, reality,...   \n",
       "8  term, line, goal, kind, happen, request, bit, ...   \n",
       "9  people, professional, fortunate, discipline, s...   \n",
       "\n",
       "                                                Text  \n",
       "0  On Tuesday at lunch, I'm part of a small group...  \n",
       "1  A variety of people have asked that I set up a...  \n",
       "2  In my copious spare time, I serve as one of th...  \n",
       "3  For as long as I've been a faculty member at G...  \n",
       "4  On Friday, I was reading the Scarlet and Black...  \n",
       "5  This past semester, I've had my first experien...  \n",
       "6  Research evidence suggests that Grinnell secon...  \n",
       "7  I've been making some bad choices recently. Th...  \n",
       "8  I rant a lot about the administration. I there...  \n",
       "9  I just returned from the 2018 SIGCSE Technical...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "musings_path = 'sam_musings/'\n",
    "musings = os.listdir(musings_path)\n",
    "\n",
    "# Import Dataset\n",
    "print('Loading musings...')\n",
    "df = pd.DataFrame({'content': [open(musings_path + musing).read() for musing in musings][0:10]})\n",
    "\n",
    "# Convert to list\n",
    "data = df.content.values.tolist()\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "print('Making bigrams...')\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "\n",
    "print('Making trigrams...')\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "print('Modding n-grams...')\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Remove Stop Words\n",
    "print('Destroying stop words...')\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "print('Forming bigrams...')\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "print('Building spacy NLP parser...')\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "print('Lemmatizing bigrams...')\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# Create Dictionary\n",
    "print('Building dictionary...')\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "print('Making corpus...')\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "topic_sizes = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100, 200]\n",
    "lda_models = [] * len(topic_sizes)\n",
    "coherence_model_ldas = [] * len(topic_sizes)\n",
    "coherence_lda = np.zeros(len(topic_sizes))\n",
    "\n",
    "# Build LDA model\n",
    "print('Generating LDA Model...')\n",
    "#for index, topics in enumerate(topic_sizes):\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                   id2word=id2word,\n",
    "                                                   num_topics=30, \n",
    "                                                   random_state=100,\n",
    "                                                   update_every=1,\n",
    "                                                   chunksize=100,\n",
    "                                                   passes=10,\n",
    "                                                   alpha='auto',\n",
    "                                                   per_word_topics=True)\n",
    "    \n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "doc_lda = lda_model[corpus]\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "# Generate mallet topic\n",
    "mallet_path = '../mallet-2.0.8/bin/mallet' # update this path\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)\n",
    "\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=32, limit=40, step=6)\n",
    "\n",
    "# Show graph\n",
    "limit=40; start=32; step=6;\n",
    "x = range(start, limit, step)\n",
    "\n",
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n",
    "\n",
    "# Select the model and print the topics\n",
    "optimal_model = model_list[np.argmax(coherence_values)]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))\n",
    "\n",
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.2927</td>\n",
       "      <td>feed, rss, title, http_www, file, gmt, content...</td>\n",
       "      <td>A variety of people have asked that I set up a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.2474</td>\n",
       "      <td>classic, artist, response, late, refer, act, n...</td>\n",
       "      <td>On Friday, I was reading the Scarlet and Black...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>major, thing, add, curriculum, question, feel,...</td>\n",
       "      <td>For as long as I've been a faculty member at G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>0.1468</td>\n",
       "      <td>computer, session, program, talk, computer_sci...</td>\n",
       "      <td>Research evidence suggests that Grinnell secon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>0.2656</td>\n",
       "      <td>term, line, goal, kind, happen, request, bit, ...</td>\n",
       "      <td>I rant a lot about the administration. I there...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Num  Topic_Perc_Contrib  \\\n",
       "0          5              0.2927   \n",
       "1          8              0.2474   \n",
       "2         11              0.1800   \n",
       "3         13              0.1468   \n",
       "4         15              0.2656   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  feed, rss, title, http_www, file, gmt, content...   \n",
       "1  classic, artist, response, late, refer, act, n...   \n",
       "2  major, thing, add, curriculum, question, feel,...   \n",
       "3  computer, session, program, talk, computer_sci...   \n",
       "4  term, line, goal, kind, happen, request, bit, ...   \n",
       "\n",
       "                                                Text  \n",
       "0  A variety of people have asked that I set up a...  \n",
       "1  On Friday, I was reading the Scarlet and Black...  \n",
       "2  For as long as I've been a faculty member at G...  \n",
       "3  Research evidence suggests that Grinnell secon...  \n",
       "4  I rant a lot about the administration. I there...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On Friday, I was reading the Scarlet and Black, Grinnell\\'s weekly newspaper [1]. In it was a short interview with the Overcoats [2], a band who are playing Grinnell on November 16. The first question is \"What kind of music did you both grow up listening to?\" Hana Elion\\'s response starts \"I liked the classics like [...]\". Now, that\\'s a phrase that many pop and rock musicians say. In the 60\\'s, the classics typically referred to artists like Elvis, Howlin\\' Wolf, Hank Williams, Junior Parker and such. By the 70\\'s, the classics were artists like The Beatles, The Stones, The Who, Motown, Otis, James Brown, Dylan, Aretha and perhaps even the Velvets. Moving forward a decade or two, one would probably add artists like Springsteen, Fleetwood Mac, Prince, and Michael Jackson. I\\'m not sure that anyone ever referred to the late 1970\\'s punk and new wave acts as \"classics\". However, by this time, it seems that Patti Smith, the Ramones, Hsker D, the Jam, the Beat, and other such acts are classics. But how did Elion continue their [3] response? \"I liked the classics like Britney Spears, Backstreet Boys, Dixie Chicks, stuff like that.\" And now I\\'m left wondering Am I just old? Is Elion just really young? Is Elion so sick of that question that they come up with absurdist replies? Is the world ending? I\\'m probably just old. I think I\\'ll just go listen to some Ella Fitzgerald. [1] I was going to say \"Oldest continuously published student newspaper west of the Mississippi\", but it no longer puts that in the title line. I\\'m not sure why it got dropped. [2] No, not the Raincoats. Didn\\'t they break up like thirty years ago? [3] I do not know what Elion prefers as a pronoun.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_topics_sorteddf_mallet['Text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopicForQuery (question):\n",
    "    temp = question.lower()\n",
    "\n",
    "    words = re.findall(r'\\w+', temp, flags = re.UNICODE | re.LOCALE)\n",
    "\n",
    "    important_words = []\n",
    "    important_words = filter(lambda x: x not in stoplist, words)\n",
    "\n",
    "    dictionary = corpora.Dictionary.load('questions.dict')\n",
    "\n",
    "    ques_vec = []\n",
    "    ques_vec = dictionary.doc2bow(important_words)\n",
    "\n",
    "    topic_vec = []\n",
    "    topic_vec = lda[ques_vec]\n",
    "\n",
    "    word_count_array = numpy.empty((len(topic_vec), 2), dtype = numpy.object)\n",
    "    for i in range(len(topic_vec)):\n",
    "        word_count_array[i, 0] = topic_vec[i][0]\n",
    "        word_count_array[i, 1] = topic_vec[i][1]\n",
    "\n",
    "    idx = numpy.argsort(word_count_array[:, 1])\n",
    "    idx = idx[::-1]\n",
    "    word_count_array = word_count_array[idx]\n",
    "\n",
    "    final = []\n",
    "    final = lda.print_topic(word_count_array[0, 0], 1)\n",
    "\n",
    "    question_topic = final.split('*') ## as format is like \"probability * topic\"\n",
    "\n",
    "    return question_topic[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'stoplist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-95836a38e48f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgetTopicForQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Hi hecker.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-b42c2fecc93f>\u001b[0m in \u001b[0;36mgetTopicForQuery\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mimportant_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mimportant_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstoplist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'questions.dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-b42c2fecc93f>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mimportant_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mimportant_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstoplist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'questions.dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'stoplist' is not defined"
     ]
    }
   ],
   "source": [
    "getTopicForQuery('Hi hecker.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
